{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from matplotlib.pylab import plt\n",
    "import plotly.graph_objects as go\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForLanguageModeling, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "import openai\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config.OPENAI_API_KEY\n",
    "client = openai.Client()\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "wandb.login(key=config.WANDB_API_KEY)\n",
    "login(token=config.HUGGINGFACE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"wandb_tutorial\"\n",
    "\n",
    "MODEL = \"llm-jp/llm-jp-1.3b-v1.0\"\n",
    "# MODE = \"llm-jp/llm-jp-3-13b\"\n",
    "DATASET = \"llm-jp/databricks-dolly-15k-ja\"\n",
    "TRAIN_SAMPLES = 10000\n",
    "EVAL_SAMPLES = 10\n",
    "EPOCHS = 5\n",
    "DEVICE_MAP = \"auto\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "LORA_RANK = 128\n",
    "LORA_ALPHA = 256\n",
    "LORA_DROPOUT = 0.1\n",
    "TARGET_MODULES = [\"c_attn\", \"c_proj\", \"c_fc\"] # llm-jp/llm-jp-1.3b-v1.0\n",
    "# TARGET_MODULES = ['gate_proj', 'up_proj', 'o_proj', 'v_proj', 'k_proj', 'q_proj', 'down_proj'] # llm-jp/llm-jp-3-13b\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "OPTIMIZER = \"paged_adamw_32bit\"\n",
    "LR_SCHEDULER_TYPE = \"cosine\"  # 学習率スケジュール\n",
    "MAX_GRAD_NORM = 0.3  # 最大法線勾配 (勾配クリッピング)\n",
    "WARMUP_RATIO = 0.1  # 線形ウォームアップのステップ比率 (0から学習率まで)\n",
    "WEIGHT_DECAY = 0.001  # bias/LayerNormウェイトを除く全レイヤーに適用するウェイト減衰\n",
    "\n",
    "NAME = f\"{MODEL}_lora_experiment\"\n",
    "NOTES = f\"{MODEL}モデルに対してLoRAを適用した実験\"\n",
    "TAGS = [MODEL, \"lora\", \"fine-tuning\"]\n",
    "\n",
    "# 実験の設定を定義\n",
    "params = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"architecture\": MODEL,\n",
    "    \"dataset\": DATASET,\n",
    "    \"train_samples\": TRAIN_SAMPLES,\n",
    "    \"eval_samples\": EVAL_SAMPLES,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"optimizer\": OPTIMIZER,\n",
    "    \"lora_rank\": LORA_RANK,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"target_modules\": TARGET_MODULES,\n",
    "    \"max_grad_norm\": MAX_GRAD_NORM,\n",
    "    \"warmup_ratio\": WARMUP_RATIO,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"lr_scheduler_type\": LR_SCHEDULER_TYPE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandbの初期化\n",
    "wandb.init(\n",
    "    project=\"wandb_tutorial\", # プロジェクト名\n",
    "    name=NAME, # 実験の名前\n",
    "    config=params, # 設定パラメータ\n",
    "    notes=NOTES, # 実験の説明\n",
    "    tags=TAGS,  # タグ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=DEVICE_MAP,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAの適用モジュールの名前を取得\n",
    "import torch\n",
    "from transformers import Conv1D\n",
    "\n",
    "def get_specific_layer_names(model):\n",
    "    # Create a list to store the layer names\n",
    "    layer_names = []\n",
    "    \n",
    "    # Recursively visit all modules and submodules\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is an instance of the specified layers\n",
    "        if isinstance(module, (torch.nn.Linear, torch.nn.Embedding, torch.nn.Conv2d, Conv1D)):\n",
    "            # model name parsing \n",
    "\n",
    "            layer_names.append('.'.join(name.split('.')[4:]).split('.')[0])\n",
    "    \n",
    "    return layer_names\n",
    "\n",
    "list(set(get_specific_layer_names(pretrained_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    instruction = f\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    sample[\"prompt\"] = prompt\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['instruction', 'context', 'response', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = dataset[\"train\"].select(range(0,params[\"train_samples\"]))\n",
    "eval_samples  = dataset[\"train\"].select(range(params[\"train_samples\"],params[\"train_samples\"]+params[\"eval_samples\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=params[\"lora_rank\"],\n",
    "    lora_alpha=params[\"lora_alpha\"],\n",
    "    target_modules=params[\"target_modules\"],\n",
    "    lora_dropout=params[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./sample_output',\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    num_train_epochs=params[\"epochs\"],\n",
    "    per_device_train_batch_size=params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=params[\"batch_size\"],\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    optim=params[\"optimizer\"],\n",
    "    lr_scheduler_type=params[\"lr_scheduler_type\"],\n",
    "    max_grad_norm=params[\"max_grad_norm\"],\n",
    "    warmup_ratio=params[\"warmup_ratio\"],\n",
    "    weight_decay=params[\"weight_decay\"],\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=eval_samples,\n",
    "    train_dataset=train_samples,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=256,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTbenchのデータを読み込む\n",
    "import json\n",
    "question_data = []\n",
    "with open(\"data/question_full.jsonl\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        question_data.append(data)\n",
    "\n",
    "gpt4_answer_data = []\n",
    "with open(\"data/gpt-4.jsonl\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        gpt4_answer_data.append(data)\n",
    "\n",
    "evaluation_dataset = []\n",
    "for question, answer in zip(question_data, gpt4_answer_data):\n",
    "    for question_turn, answer_turn in zip(question[\"turns\"], answer[\"choices\"][0][\"turns\"]):\n",
    "        evaluation_dataset.append({\"question\": question_turn, \"answer\": answer_turn, \"category\": question[\"category\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt_data = []\n",
    "with open(\"prompt/judge_ja_prompts.jsonl\", \"r\") as fin:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        judge_prompt_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationScore(BaseModel):\n",
    "    evaluation_reason: str\n",
    "    score: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def mtbench_score(question: str, answer: str, model_name: str, reference_answer: str=\"\", **kwargs) -> dict:\n",
    "    if reference_answer:\n",
    "        judge_prompt = judge_prompt_data[1][\"prompt_template\"]\n",
    "        judge_prompt = judge_prompt.format(question=question, answer=answer, ref_answer_1=reference_answer)\n",
    "    else:\n",
    "        judge_prompt = judge_prompt_data[0][\"prompt_template\"]\n",
    "        judge_prompt = judge_prompt.format(question=question, answer=answer)\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": judge_prompt},\n",
    "        ],\n",
    "        response_format=EvaluationScore,\n",
    "        **kwargs\n",
    "    )\n",
    "    return {\"score\": response.choices[0].message.parsed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, tokenizer, model):\n",
    "    try:\n",
    "        input_ids = tokenizer(prompt, add_special_tokens=False, return_tensors='pt').to(model.device)\n",
    "        output_ids = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = tokenizer.batch_decode(output_ids[:, input_ids['input_ids'].shape[1]:], skip_special_tokens=True)[0].strip()\n",
    "    except:\n",
    "        input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(model.device)\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        generated_text = tokenizer.decode(output_ids[0])\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sample in evaluation_dataset:\n",
    "    generated_text = generate_text(sample[\"question\"], tokenizer, finetuned_model)\n",
    "    result = mtbench_score(question=sample[\"question\"], answer=generated_text, model_name=\"gpt-4o\", reference_answer=sample[\"answer\"], temperature=0.01)\n",
    "    results.append({\n",
    "        \"入力\": sample[\"question\"],\n",
    "        \"生成結果\": generated_text,\n",
    "        \"正解文\": sample[\"answer\"],\n",
    "        \"MTBench_判定理由\": result[\"score\"].evaluation_reason,\n",
    "        \"MTBench_スコア\": result[\"score\"].score,\n",
    "        \"MTBench_カテゴリ\": sample[\"category\"]\n",
    "    })\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df[\"model_name\"] = MODEL\n",
    "df[\"実験名\"] = NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.use_artifact(\"mtbench_score_artifact:latest\", type=\"dataset\")\n",
    "artifact_dir = artifact.download()\n",
    "with open(f\"{artifact_dir}/mtbench_score_key.table.json\") as f:\n",
    "    tjs = json.load(f)\n",
    "output_table = wandb.Table.from_json(json_obj=tjs, source_artifact=artifact)\n",
    "output_df = pd.DataFrame(data=output_table.data, columns=output_table.columns)\n",
    "output_df = pd.concat([output_df, df], ignore_index=True)\n",
    "artifact = wandb.Artifact(\"mtbench_score_artifact\", type=\"dataset\")\n",
    "artifact.add(wandb.Table(dataframe=output_df), \"mtbench_score_key\")  \n",
    "# あるいは df.to_csv() してファイルとして add_file() するなど\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = output_df.groupby([\"MTBench_カテゴリ\", \"実験名\"])[\"MTBench_スコア\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for experiment_name in radar_df[\"実験名\"].unique():\n",
    "    radar_df_ = radar_df[radar_df[\"実験名\"] == experiment_name]\n",
    "    categories = radar_df_[\"MTBench_カテゴリ\"].unique().tolist()\n",
    "    values = radar_df_[\"MTBench_スコア\"].values.tolist()\n",
    "    fig.add_trace(\n",
    "    go.Scatterpolar(\n",
    "        r=values,\n",
    "        theta=categories,\n",
    "        fill='toself',   # 内側を塗りつぶし\n",
    "            name=experiment_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 10]  # 値に合わせてrangeを調整\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True\n",
    ")\n",
    "wandb.log({\"MTBench_スコア\": fig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
